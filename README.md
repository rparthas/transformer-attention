# Transformer: Educational PyTorch Implementation

A toy implementation of the Transformer architecture from the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).

## ğŸ¯ Learning Objectives

This project is designed for **educational purposes** to understand the Transformer architecture through implementation. The focus is on:

- **Clarity over Performance**: Code is written to mirror the paper's equations explicitly
- **Small Scale**: Toy model dimensions (d_model=128, h=4 heads) for fast CPU training
- **Comprehensive Comments**: Extensive documentation explaining the "why" behind each component
- **Progressive Learning**: Build complexity incrementally from attention mechanisms to full model

## ğŸ“š What You'll Learn

1. **Scaled Dot-Product Attention**: The fundamental attention mechanism
2. **Multi-Head Attention**: Parallel attention heads for diverse representations
3. **Positional Encoding**: Injecting sequence order information
4. **Encoder Architecture**: Self-attention and feed-forward layers
5. **Decoder Architecture**: Masked attention and cross-attention
6. **Training**: Adam optimizer, learning rate scheduling, label smoothing
7. **Validation**: Simple tasks (copy, reverse) to verify implementation

## ğŸš€ Quick Start

### Installation

```bash
# Using uv package manager
uv sync

# Or manually with pip
pip install -r requirements.txt
```

### Project Structure

```
transformer-attention/
â”œâ”€â”€ transformer/          # Main implementation
â”‚   â”œâ”€â”€ attention.py     # Scaled dot-product & multi-head attention
â”‚   â”œâ”€â”€ layers.py        # Encoder & decoder layers
â”‚   â”œâ”€â”€ model.py         # Complete Transformer model
â”‚   â”œâ”€â”€ positional.py    # Positional encodings
â”‚   â””â”€â”€ utils.py         # Helper functions
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ notebooks/           # Jupyter notebooks for exploration
â”œâ”€â”€ pyproject.toml       # Project configuration
â””â”€â”€ README.md           # This file
```

### Running Tests

```bash
pytest tests/
```

## ğŸ“– Implementation Roadmap

This project follows a structured implementation plan across 13 stories:

### Foundation (Stories 1-6)
- âœ… Story 01: Project Setup
- âœ… Story 02: Scaled Dot-Product Attention
- âœ… Story 03: Multi-Head Attention
- âœ… Story 04: Positional Encoding
- â³ Story 05: Feed-Forward Network
- â³ Story 06: Encoder Layer

### Architecture Assembly (Stories 7-10)
- â³ Story 07: Encoder Stack
- â³ Story 08: Decoder Layer
- â³ Story 09: Decoder Stack
- â³ Story 10: Full Transformer Model

### Training & Validation (Stories 11-13)
- â³ Story 11: Training Loop
- â³ Story 12: Toy Datasets (Copy/Reverse Tasks)
- â³ Story 13: Attention Visualization

## ğŸ“ Key Differences from Production Models

| Aspect | This Implementation | Paper/Production |
|--------|-------------------|------------------|
| d_model | 128 | 512 |
| num_heads | 4 | 8 |
| num_layers | 2 | 6 |
| d_ff | 512 | 2048 |
| vocab_size | 1000 | 30,000+ |
| Training | Toy tasks (copy/reverse) | WMT translation |
| Hardware | CPU | Multi-GPU |

## ğŸ“ References

- **Paper**: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- **Annotated Transformer**: [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- **PyTorch Documentation**: [torch.nn](https://pytorch.org/docs/stable/nn.html)

## ğŸ¤ Learning Philosophy

This implementation prioritizes:
1. **Understanding**: Extensive comments explaining mathematical intuitions
2. **Testability**: Each component has unit tests with shape validation
3. **Modularity**: Clear separation of concerns for easy debugging
4. **Reproducibility**: Fixed random seeds and deterministic operations

## ğŸ“Š Expected Results

When trained on toy tasks:
- **Copy Task**: >99% accuracy within 5 epochs
- **Reverse Task**: >95% accuracy within 10 epochs

If these benchmarks aren't met, the implementation needs debugging!

## ğŸ”§ Development

```bash
# Run tests
pytest tests/ -v

# Run specific test file
pytest tests/test_attention.py

# Generate coverage report
pytest --cov=transformer tests/
```

## ğŸ“„ License

Educational project - use freely for learning purposes.

## ğŸ™ Acknowledgments

Based on the groundbreaking work by Vaswani et al. in "Attention Is All You Need".
